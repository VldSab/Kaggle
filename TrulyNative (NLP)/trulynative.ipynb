{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport zipfile\nimport re\nfrom bs4 import BeautifulSoup","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-03T18:37:23.250707Z","iopub.execute_input":"2022-03-03T18:37:23.251448Z","iopub.status.idle":"2022-03-03T18:37:23.580947Z","shell.execute_reply.started":"2022-03-03T18:37:23.251308Z","shell.execute_reply":"2022-03-03T18:37:23.580024Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import f1_score, accuracy_score","metadata":{"execution":{"iopub.status.busy":"2022-03-03T18:47:12.635743Z","iopub.execute_input":"2022-03-03T18:47:12.636039Z","iopub.status.idle":"2022-03-03T18:47:12.643005Z","shell.execute_reply.started":"2022-03-03T18:47:12.636007Z","shell.execute_reply":"2022-03-03T18:47:12.641984Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"!pip install transformers==2.11.0","metadata":{"execution":{"iopub.status.busy":"2022-03-03T18:37:24.344832Z","iopub.execute_input":"2022-03-03T18:37:24.345974Z","iopub.status.idle":"2022-03-03T18:37:41.090180Z","shell.execute_reply.started":"2022-03-03T18:37:24.345923Z","shell.execute_reply":"2022-03-03T18:37:41.088973Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import transformers","metadata":{"execution":{"iopub.status.busy":"2022-03-03T18:37:41.094483Z","iopub.execute_input":"2022-03-03T18:37:41.094770Z","iopub.status.idle":"2022-03-03T18:37:50.745346Z","shell.execute_reply.started":"2022-03-03T18:37:41.094738Z","shell.execute_reply":"2022-03-03T18:37:50.744133Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"print(transformers.__version__)","metadata":{"execution":{"iopub.status.busy":"2022-03-03T18:37:50.747621Z","iopub.execute_input":"2022-03-03T18:37:50.748000Z","iopub.status.idle":"2022-03-03T18:37:50.760768Z","shell.execute_reply.started":"2022-03-03T18:37:50.747946Z","shell.execute_reply":"2022-03-03T18:37:50.758891Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertConfig, get_linear_schedule_with_warmup\nfrom torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler","metadata":{"execution":{"iopub.status.busy":"2022-03-03T18:37:50.765873Z","iopub.execute_input":"2022-03-03T18:37:50.766284Z","iopub.status.idle":"2022-03-03T18:37:50.777346Z","shell.execute_reply.started":"2022-03-03T18:37:50.766248Z","shell.execute_reply":"2022-03-03T18:37:50.776343Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import random\nimport warnings\nimport time\nimport datetime","metadata":{"execution":{"iopub.status.busy":"2022-03-03T18:37:50.779132Z","iopub.execute_input":"2022-03-03T18:37:50.779603Z","iopub.status.idle":"2022-03-03T18:37:50.788218Z","shell.execute_reply.started":"2022-03-03T18:37:50.779530Z","shell.execute_reply":"2022-03-03T18:37:50.787155Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/dato-native/train_v2.csv.zip\")\ndf","metadata":{"execution":{"iopub.status.busy":"2022-03-03T18:37:50.790463Z","iopub.execute_input":"2022-03-03T18:37:50.790700Z","iopub.status.idle":"2022-03-03T18:37:51.161351Z","shell.execute_reply.started":"2022-03-03T18:37:50.790670Z","shell.execute_reply":"2022-03-03T18:37:51.160392Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"print(transformers.__version__)","metadata":{"execution":{"iopub.status.busy":"2022-03-03T18:37:51.163443Z","iopub.execute_input":"2022-03-03T18:37:51.163774Z","iopub.status.idle":"2022-03-03T18:37:51.175595Z","shell.execute_reply.started":"2022-03-03T18:37:51.163729Z","shell.execute_reply":"2022-03-03T18:37:51.170465Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"#create dict {filename: text}\ndef zip_to_textdict(path: str):\n    archive = zipfile.ZipFile(path)\n    return {name: archive.read(name) for name in archive.namelist()}\n\n#find file by name and read it\ndef get_text_by_name(file_name: str):\n    list_of_archives = ['../input/dato-native/0.zip', '../input/dato-native/1.zip','../input/dato-native/2.zip', \n                    '../input/dato-native/3.zip', '../input/dato-native/4.zip', '../input/dato-native/5.zip']\n    for archive_path in list_of_archives:\n        archive = zipfile.ZipFile(archive_path)\n        prefix = archive_path.split('/')[-1][0]\n        fname = prefix + '/' + file_name\n        print(fname)\n        if fname in archive.namelist():\n            return archive.read(fname)\n        \n#delete text, punctuation, links etc.        \ndef clean_text(text):\n    soup = BeautifulSoup(text)\n    text = soup.get_text(separator=' ')\n    clean_url = re.compile(r'https?://\\S+|www\\.\\S+')\n    text = re.sub(clean_url, ' ', text)\n    clean_html = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n    text_nohtml = re.sub(clean_html, ' ', text)\n    clean_punctuation = re.compile(r'[^\\w\\s]')\n    text_nopunct = re.sub(clean_punctuation, ' ', text_nohtml)\n    change_to_space = [' n ', ' a ', ' b ', ' c ', ' d ', ' e ', ' f ', ' j ', ' h ', ' i ', ' j ', ' k ', ' l ', ' m ', ' o ',\n                       ' p ', ' q ', ' r ', ' s ', ' t ', ' u ', ' v ', ' w ', ' x ', ' y ', ' z ', '0', '1', '2', '3', '4',\n                      '5', '6', '7', '8', '9']\n    for item in change_to_space:\n        string = '({item})+'.format(item=item)\n        clean_n = re.compile(string)\n        text_nopunct = re.sub(clean_n, ' ', text_nopunct)\n        #text_nospaces = text_nospaces.replace(item, ' ')\n    delete = ['if', 'else', 'div', 'var', 'src', 'url', 'return', 'function', 'class', 'None', 'item', 'in', 'shift', 'strg',\n             'split', 'hgroup', 'nodeName', 'imports', 'typeof', 'footer', 'createElem', 'addPrefix', 'scriptTag', 'itialize',\n              'script', 'try', 'setAttribute', 'http', 'https', 'href', ';']\n    for item in delete:\n        text_nopunct = text_nopunct.replace(item, '')\n        \n    clean_spaces = re.compile(r'\\s+')\n    text_nospaces = re.sub(clean_spaces, ' ', text_nopunct).strip()\n    return text_nospaces\n\n#dict {filename: tokenized_text }\ndef zip_to_tokensdict(path: str):\n    archive = zipfile.ZipFile(path)\n    tokens_dict = {}\n    for i, name in enumerate(archive.namelist()):\n        if i % 100 == 0:\n            print(i)\n        # because 0th elem is not txt\n        if i == 0: continue\n        text = str(archive.read(name))\n        text = clean_text(text)\n        tokenized_text = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))\n        tokens_dict[name] = tokenized_text\n    return tokens_dict\n\n# write line to csv\ndef write_csv(csv_name, args):\n    with open(csv_name,'a') as f:\n        f.write(args[0] + ';' + args[1] + '\\n')\n    \n      \n# create csv name|text\ndef name_to_text(archive_path: str, csv_name: str):\n    archive = zipfile.ZipFile(archive_path)\n    for i, name in enumerate(archive.namelist()):\n        if i % 100 == 0:\n            print(i)\n        text = clean_text(archive.read(name))\n        write_csv(csv_name, [name, text])","metadata":{"execution":{"iopub.status.busy":"2022-03-03T18:37:51.178234Z","iopub.execute_input":"2022-03-03T18:37:51.178906Z","iopub.status.idle":"2022-03-03T18:37:51.242542Z","shell.execute_reply.started":"2022-03-03T18:37:51.178697Z","shell.execute_reply":"2022-03-03T18:37:51.240769Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"list_of_archives = ['../input/dato-native/0.zip', '../input/dato-native/1.zip','../input/dato-native/2.zip', \n                    '../input/dato-native/3.zip', '../input/dato-native/4.zip', '../input/dato-native/5.zip']","metadata":{"execution":{"iopub.status.busy":"2022-03-03T18:41:17.843988Z","iopub.execute_input":"2022-03-03T18:41:17.844327Z","iopub.status.idle":"2022-03-03T18:41:17.849860Z","shell.execute_reply.started":"2022-03-03T18:41:17.844285Z","shell.execute_reply":"2022-03-03T18:41:17.848526Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/nametext/name-text0.csv', delimiter=';')\ndf.iloc[:, 0] = df.iloc[:, 0].apply(lambda x: x[2:])\ndf.columns = ['file', 'text']\ndf","metadata":{"execution":{"iopub.status.busy":"2022-03-03T18:41:18.064431Z","iopub.execute_input":"2022-03-03T18:41:18.064719Z","iopub.status.idle":"2022-03-03T18:41:20.930539Z","shell.execute_reply.started":"2022-03-03T18:41:18.064688Z","shell.execute_reply":"2022-03-03T18:41:20.929519Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"labels_df = pd.read_csv(\"../input/dato-native/train_v2.csv.zip\")\nlabels_df","metadata":{"execution":{"iopub.status.busy":"2022-03-03T18:41:20.932898Z","iopub.execute_input":"2022-03-03T18:41:20.933226Z","iopub.status.idle":"2022-03-03T18:41:21.215994Z","shell.execute_reply.started":"2022-03-03T18:41:20.933179Z","shell.execute_reply":"2022-03-03T18:41:21.214801Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"df = df.merge(labels_df, how='left')\ndf.dropna(subset = [\"text\"], inplace=True)\ndf","metadata":{"execution":{"iopub.status.busy":"2022-03-03T18:41:21.218481Z","iopub.execute_input":"2022-03-03T18:41:21.218866Z","iopub.status.idle":"2022-03-03T18:41:21.494262Z","shell.execute_reply.started":"2022-03-03T18:41:21.218787Z","shell.execute_reply":"2022-03-03T18:41:21.493194Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"train_text = df.text.values\nlabels = df.sponsored.values\nprint(len(train_text))\nidx = 40000","metadata":{"execution":{"iopub.status.busy":"2022-03-03T18:41:21.516140Z","iopub.execute_input":"2022-03-03T18:41:21.517667Z","iopub.status.idle":"2022-03-03T18:41:21.528288Z","shell.execute_reply.started":"2022-03-03T18:41:21.517615Z","shell.execute_reply":"2022-03-03T18:41:21.526802Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-large-uncased', do_lower_case=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-03T18:41:22.487486Z","iopub.execute_input":"2022-03-03T18:41:22.488342Z","iopub.status.idle":"2022-03-03T18:41:22.989597Z","shell.execute_reply.started":"2022-03-03T18:41:22.488285Z","shell.execute_reply":"2022-03-03T18:41:22.988600Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"train= train_text[:idx]\ntest = train_text[idx:]\ntrain.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-03T18:41:24.537811Z","iopub.execute_input":"2022-03-03T18:41:24.538460Z","iopub.status.idle":"2022-03-03T18:41:24.545569Z","shell.execute_reply.started":"2022-03-03T18:41:24.538423Z","shell.execute_reply":"2022-03-03T18:41:24.544460Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"labels_train = labels[:idx]\nlabels_test = labels[idx:]","metadata":{"execution":{"iopub.status.busy":"2022-03-03T18:41:25.386567Z","iopub.execute_input":"2022-03-03T18:41:25.387332Z","iopub.status.idle":"2022-03-03T18:41:25.393019Z","shell.execute_reply.started":"2022-03-03T18:41:25.387278Z","shell.execute_reply":"2022-03-03T18:41:25.392009Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"def tokenize_map(sentence,labs='None'):\n    \n    \"\"\"A function for tokenize all of the sentences and map the tokens to their word IDs.\"\"\"\n    \n    global labels_train\n    \n    input_ids = []\n    attention_masks = []\n\n    # For every sentence...\n    \n    for i, text in enumerate(sentence):\n        if i % 100 == 0:\n            print(i)\n        #   \"encode_plus\" will:\n        \n        #   (1) Tokenize the sentence.\n        #   (2) Prepend the `[CLS]` token to the start.\n        #   (3) Append the `[SEP]` token to the end.\n        #   (4) Map tokens to their IDs.\n        #   (5) Pad or truncate the sentence to `max_length`\n        #   (6) Create attention masks for [PAD] tokens.\n        \n        encoded_dict = tokenizer.encode_plus(\n                            text,                      # Sentence to encode.\n                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                            truncation='longest_first', # Activate and control truncation\n                            max_length = 256,           # Max length according to our text data.\n                            pad_to_max_length = True, # Pad & truncate all sentences.\n                            return_attention_mask = True,   # Construct attn. masks.\n                            return_tensors = 'pt',     # Return pytorch tensors.\n                       )\n\n        # Add the encoded sentence to the id list. \n        \n        input_ids.append(encoded_dict['input_ids'])\n\n        # And its attention mask (simply differentiates padding from non-padding).\n        \n        attention_masks.append(encoded_dict['attention_mask'])\n\n    # Convert the lists into tensors.\n    \n    input_ids = torch.cat(input_ids, dim=0)\n    attention_masks = torch.cat(attention_masks, dim=0)\n    \n    if labs != 'None': # Setting this for using this definition for both train and test data so labels won't be a problem in our outputs.\n        labels = torch.tensor(labels_train)\n        return input_ids, attention_masks, labels\n    else:\n        return input_ids, attention_masks","metadata":{"execution":{"iopub.status.busy":"2022-03-03T18:41:26.340110Z","iopub.execute_input":"2022-03-03T18:41:26.340770Z","iopub.status.idle":"2022-03-03T18:41:26.351947Z","shell.execute_reply.started":"2022-03-03T18:41:26.340733Z","shell.execute_reply":"2022-03-03T18:41:26.350929Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"input_ids, attention_masks, labels = tokenize_map(train[:500], labels_train[:500])","metadata":{"execution":{"iopub.status.busy":"2022-03-03T18:41:32.778351Z","iopub.execute_input":"2022-03-03T18:41:32.779219Z","iopub.status.idle":"2022-03-03T18:42:30.129036Z","shell.execute_reply.started":"2022-03-03T18:41:32.779183Z","shell.execute_reply":"2022-03-03T18:42:30.127280Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"len(input_ids)","metadata":{"execution":{"iopub.status.busy":"2022-03-03T18:42:30.131525Z","iopub.execute_input":"2022-03-03T18:42:30.132198Z","iopub.status.idle":"2022-03-03T18:42:30.140278Z","shell.execute_reply.started":"2022-03-03T18:42:30.132151Z","shell.execute_reply":"2022-03-03T18:42:30.139024Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"len(labels)","metadata":{"execution":{"iopub.status.busy":"2022-03-03T18:42:48.815299Z","iopub.execute_input":"2022-03-03T18:42:48.815631Z","iopub.status.idle":"2022-03-03T18:42:48.822676Z","shell.execute_reply.started":"2022-03-03T18:42:48.815599Z","shell.execute_reply":"2022-03-03T18:42:48.821088Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"#for testing\nlabels = labels_train[:500]\nlabels = torch.tensor(labels)\nlabels.to(torch.int32)\nlabels = torch.nan_to_num(labels, nan=0)\nlabels = torch.tensor(labels, dtype=torch.int32)","metadata":{"execution":{"iopub.status.busy":"2022-03-03T18:42:50.150060Z","iopub.execute_input":"2022-03-03T18:42:50.151083Z","iopub.status.idle":"2022-03-03T18:42:50.159880Z","shell.execute_reply.started":"2022-03-03T18:42:50.151008Z","shell.execute_reply":"2022-03-03T18:42:50.158637Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"labels","metadata":{"execution":{"iopub.status.busy":"2022-03-03T18:42:53.096016Z","iopub.execute_input":"2022-03-03T18:42:53.097159Z","iopub.status.idle":"2022-03-03T18:42:53.109453Z","shell.execute_reply.started":"2022-03-03T18:42:53.097109Z","shell.execute_reply":"2022-03-03T18:42:53.108475Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"test_input_ids, test_attention_masks = tokenize_map(test[:100])","metadata":{"execution":{"iopub.status.busy":"2022-03-03T18:42:58.952573Z","iopub.execute_input":"2022-03-03T18:42:58.953035Z","iopub.status.idle":"2022-03-03T18:43:05.246696Z","shell.execute_reply.started":"2022-03-03T18:42:58.953000Z","shell.execute_reply":"2022-03-03T18:43:05.245642Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"# Combine the training inputs into a TensorDataset.\n\ndataset = TensorDataset(input_ids, attention_masks, labels)\n\n# Create a 80-20 train-validation split.\n\n# Calculate the number of samples to include in each set.\n\ntrain_size = int(0.8 * len(dataset))\nval_size = len(dataset) - train_size\n\n# Divide the dataset by randomly selecting samples.\n\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\nprint('{:>5,} training samples'.format(train_size))\nprint('{:>5,} validation samples'.format(val_size))","metadata":{"execution":{"iopub.status.busy":"2022-03-03T18:43:05.248839Z","iopub.execute_input":"2022-03-03T18:43:05.249169Z","iopub.status.idle":"2022-03-03T18:43:05.258985Z","shell.execute_reply.started":"2022-03-03T18:43:05.249123Z","shell.execute_reply":"2022-03-03T18:43:05.257586Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache() ","metadata":{"execution":{"iopub.status.busy":"2022-03-03T18:43:05.366874Z","iopub.execute_input":"2022-03-03T18:43:05.367190Z","iopub.status.idle":"2022-03-03T18:43:05.372307Z","shell.execute_reply.started":"2022-03-03T18:43:05.367157Z","shell.execute_reply":"2022-03-03T18:43:05.371194Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"# The DataLoader needs to know our batch size for training, so we specify it here. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32.\n\nbatch_size = 2\n\n# Create the DataLoaders for our training and validation sets.\n# We'll take training samples in random order. \n\ntrain_dataloader = DataLoader(\n            train_dataset,  # The training samples.\n            sampler = RandomSampler(train_dataset), # Select batches randomly\n            batch_size = batch_size # Trains with this batch size.\n        )\n\n# For validation the order doesn't matter, so we'll just read them sequentially.\n\nvalidation_dataloader = DataLoader(\n            val_dataset, # The validation samples.\n            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n            batch_size = batch_size # Evaluate with this batch size.\n        )","metadata":{"execution":{"iopub.status.busy":"2022-03-03T18:43:06.915255Z","iopub.execute_input":"2022-03-03T18:43:06.915584Z","iopub.status.idle":"2022-03-03T18:43:06.923158Z","shell.execute_reply.started":"2022-03-03T18:43:06.915550Z","shell.execute_reply":"2022-03-03T18:43:06.921749Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"prediction_data = TensorDataset(test_input_ids, test_attention_masks)\nprediction_sampler = SequentialSampler(prediction_data)\nprediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2022-03-03T18:43:10.316588Z","iopub.execute_input":"2022-03-03T18:43:10.317006Z","iopub.status.idle":"2022-03-03T18:43:10.328303Z","shell.execute_reply.started":"2022-03-03T18:43:10.316933Z","shell.execute_reply":"2022-03-03T18:43:10.327297Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"# If there's a GPU available...\n\nif torch.cuda.is_available():    \n\n    # Tell PyTorch to use the GPU.  \n    \n    device = torch.device('cuda')    \n\n\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n\n# If not...\n\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device('cpu')","metadata":{"execution":{"iopub.status.busy":"2022-03-03T18:44:19.158312Z","iopub.execute_input":"2022-03-03T18:44:19.158742Z","iopub.status.idle":"2022-03-03T18:44:19.166898Z","shell.execute_reply.started":"2022-03-03T18:44:19.158693Z","shell.execute_reply":"2022-03-03T18:44:19.165759Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n\nmodel = BertForSequenceClassification.from_pretrained(\n    'bert-large-uncased', # Use the 124-layer, 1024-hidden, 16-heads, 340M parameters BERT model with an uncased vocab.\n    num_labels = 2, # The number of output labels--2 for binary classification. You can increase this for multi-class tasks.   \n    output_attentions = False, # Whether the model returns attentions weights.\n    output_hidden_states = False, # Whether the model returns all hidden-states.\n)\n\n# Tell pytorch to run this model on the device which we set GPU in our case.\n\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-03-03T18:39:39.522733Z","iopub.execute_input":"2022-03-03T18:39:39.523059Z","iopub.status.idle":"2022-03-03T18:41:01.979254Z","shell.execute_reply.started":"2022-03-03T18:39:39.523025Z","shell.execute_reply":"2022-03-03T18:41:01.978296Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# Note: AdamW is a class from the huggingface library (as opposed to pytorch).\n\n# The 'W' stands for 'Weight Decay fix' probably...\n\noptimizer = AdamW(model.parameters(),\n                  lr = 6e-6, # args.learning_rate\n                  eps = 1e-8 # args.adam_epsilon\n                )","metadata":{"execution":{"iopub.status.busy":"2022-03-03T18:44:32.451843Z","iopub.execute_input":"2022-03-03T18:44:32.452137Z","iopub.status.idle":"2022-03-03T18:44:32.460852Z","shell.execute_reply.started":"2022-03-03T18:44:32.452105Z","shell.execute_reply":"2022-03-03T18:44:32.459653Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"# Number of training epochs. The BERT authors recommend between 2 and 4. \n\n# We chose to run for 3, but we'll see later that this may be over-fitting the training data.\n\nepochs = 1\n\n# Total number of training steps is [number of batches] x [number of epochs] (Note that this is not the same as the number of training samples).\ntotal_steps = len(train_dataloader) * epochs\n\n# Create the learning rate scheduler.\n\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps = 0, # Default value in run_glue.py\n                                            num_training_steps = total_steps)","metadata":{"execution":{"iopub.status.busy":"2022-03-03T18:44:33.159224Z","iopub.execute_input":"2022-03-03T18:44:33.159568Z","iopub.status.idle":"2022-03-03T18:44:33.166870Z","shell.execute_reply.started":"2022-03-03T18:44:33.159535Z","shell.execute_reply":"2022-03-03T18:44:33.165743Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"def flat_accuracy(preds, labels):\n    \n    \"\"\"A function for calculating accuracy scores\"\"\"\n    \n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    \n    return accuracy_score(labels_flat, pred_flat)\n\ndef flat_f1(preds, labels):\n    \n    \"\"\"A function for calculating f1 scores\"\"\"\n    \n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    \n    return f1_score(labels_flat, pred_flat)","metadata":{"execution":{"iopub.status.busy":"2022-03-03T18:44:36.160879Z","iopub.execute_input":"2022-03-03T18:44:36.161187Z","iopub.status.idle":"2022-03-03T18:44:36.168225Z","shell.execute_reply.started":"2022-03-03T18:44:36.161153Z","shell.execute_reply":"2022-03-03T18:44:36.167068Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"def format_time(elapsed):    \n    \n    \"\"\"A function that takes a time in seconds and returns a string hh:mm:ss\"\"\"\n    \n    # Round to the nearest second.\n    elapsed_rounded = int(round((elapsed)))\n    \n    # Format as hh:mm:ss\n    return str(datetime.timedelta(seconds=elapsed_rounded))","metadata":{"execution":{"iopub.status.busy":"2022-03-03T18:44:36.170749Z","iopub.execute_input":"2022-03-03T18:44:36.171423Z","iopub.status.idle":"2022-03-03T18:44:36.181814Z","shell.execute_reply.started":"2022-03-03T18:44:36.171360Z","shell.execute_reply":"2022-03-03T18:44:36.180613Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"labels","metadata":{"execution":{"iopub.status.busy":"2022-03-03T18:44:38.751108Z","iopub.execute_input":"2022-03-03T18:44:38.751824Z","iopub.status.idle":"2022-03-03T18:44:38.765031Z","shell.execute_reply.started":"2022-03-03T18:44:38.751783Z","shell.execute_reply":"2022-03-03T18:44:38.763814Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"# This training code is based on the `run_glue.py` script here:\n\n# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n\n# We'll store a number of quantities such as training and validation loss, validation accuracy, f1 score and timings.\n\ntraining_stats = []\n\n# Measure the total training time for the whole run.\n\ntotal_t0 = time.time()\n\n# For each epoch...\n\nfor epoch_i in range(0, epochs):\n    \n    # ========================================\n    #               Training\n    # ========================================\n    \n    # Perform one full pass over the training set.\n\n    print('')\n    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n    print('Training...')\n\n    # Measure how long the training epoch takes:\n    \n    t0 = time.time()\n\n    # Reset the total loss for this epoch.\n    \n    total_train_loss = 0\n\n    # Put the model into training mode. Don't be mislead--the call to `train` just changes the *mode*, it doesn't *perform* the training.\n    \n    # `dropout` and `batchnorm` layers behave differently during training vs. test ,\n    # source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch\n    \n    model.train()\n\n    # For each batch of training data...\n    \n    for step, batch in enumerate(train_dataloader):\n\n        # Progress update every 50 batches.\n        if step % 50 == 0 and not step == 0:\n            # Calculate elapsed time in minutes.\n            elapsed = format_time(time.time() - t0)\n            \n            # Report progress.\n            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n\n        # Unpack this training batch from our dataloader. \n        #\n        # As we unpack the batch, we'll also copy each tensor to the device(gpu in our case) using the `to` method.\n        #\n        # `batch` contains three pytorch tensors:\n        #   [0]: input ids \n        #   [1]: attention masks\n        #   [2]: labels \n        \n        b_input_ids = batch[0].to(device).to(torch.int64)\n        b_input_mask = batch[1].to(device).to(torch.int64)\n        b_labels = batch[2].to(device).to(torch.int64)\n        b_labels = torch.tensor(b_labels, dtype=torch.long)\n        print(b_labels)\n        # Always clear any previously calculated gradients before performing a backward pass. PyTorch doesn't do this automatically because accumulating the gradients is 'convenient while training RNNs'. \n        # Source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch\n        \n        model.zero_grad()        \n\n        # Perform a forward pass (evaluate the model on this training batch).\n        # The documentation for this `model` function is down here: \n        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers BertForSequenceClassification.\n        \n        # It returns different numbers of parameters depending on what arguments given and what flags are set. For our useage here, it returns the loss (because we provided labels),\n        # And the 'logits' (the model outputs prior to activation.)\n        \n        loss, logits = model(b_input_ids, \n                             token_type_ids=None, \n                             attention_mask=b_input_mask, \n                             labels=b_labels)\n\n        # Accumulate the training loss over all of the batches so that we can calculate the average loss at the end, \n        # `loss` is a tensor containing a single value; the `.item()` function just returns the Python value from the tensor.\n        total_train_loss += loss.item()\n\n        # Perform a backward pass to calculate the gradients.\n        \n        loss.backward()\n\n        # Clip the norm of the gradients to 1.0 This is to help prevent the 'exploding gradients' problem.\n        \n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        # Update parameters and take a step using the computed gradient.\n        \n        # The optimizer dictates the 'update rule'(How the parameters are modified based on their gradients, the learning rate, etc.)\n        \n        optimizer.step()\n\n        # Update the learning rate.\n        \n        scheduler.step()\n\n    # Calculate the average loss over all of the batches.\n    \n    avg_train_loss = total_train_loss / len(train_dataloader)            \n    \n    # Measure how long this epoch took.\n    \n    training_time = format_time(time.time() - t0)\n\n    print('')\n    print('  Average training loss: {0:.2f}'.format(avg_train_loss))\n    print('  Training epcoh took: {:}'.format(training_time))\n        \n    # ========================================\n    #               Validation\n    # ========================================\n    # After the completion of each training epoch, measure our performance on our validation set.\n\n    print('')\n    print('Running Validation...')\n\n    t0 = time.time()\n\n    # Put the model in evaluation mode--the dropout layers behave differently during evaluation.\n    \n    model.eval()\n\n    # Tracking variables:\n    \n    total_eval_accuracy = 0\n    total_eval_loss = 0\n    total_eval_f1 = 0\n    nb_eval_steps = 0\n\n    # Evaluate data for one epoch.\n    \n    for batch in validation_dataloader:\n        \n        # Unpack this training batch from our dataloader. \n        \n        # As we unpack the batch, we'll also copy each tensor to the GPU using the `to` method.\n        \n        # `batch` contains three pytorch tensors:\n        #   [0]: input ids \n        #   [1]: attention masks\n        #   [2]: labels \n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n        print(b_labels)\n        b_labels = torch.tensor(b_labels, dtype=torch.long)\n        # Tell pytorch not to bother with constructing the compute graph during the forward pass, since this is only needed for backprop (training part).\n        \n        with torch.no_grad():        \n\n            # Forward pass, calculate logit predictions.\n            # token_type_ids is the same as the 'segment ids', which differentiates sentence 1 and 2 in 2-sentence tasks.\n            # The documentation for this `model` function is down here: \n            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers BertForSequenceClassification.\n            # Get the 'logits' output by the model. The 'logits' are the output values prior to applying an activation function like the softmax.\n            \n            (loss, logits) = model(b_input_ids, \n                                   token_type_ids=None, \n                                   attention_mask=b_input_mask,\n                                   labels=b_labels)\n            \n        # Accumulate the validation loss.\n        \n        total_eval_loss += loss.item()\n\n        # Move logits and labels to CPU:\n        \n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n\n        # Calculate the accuracy for this batch of test sentences, and accumulate it over all batches:\n        \n        total_eval_accuracy += flat_accuracy(logits, label_ids)\n        total_eval_f1 += flat_f1(logits, label_ids)\n        \n\n    # Report the final accuracy for this validation run.\n    \n    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n    print('  Accuracy: {0:.2f}'.format(avg_val_accuracy))\n    \n    # Report the final f1 score for this validation run.\n    \n    avg_val_f1 = total_eval_f1 / len(validation_dataloader)\n    print('  F1: {0:.2f}'.format(avg_val_f1))\n\n    # Calculate the average loss over all of the batches.\n    \n    avg_val_loss = total_eval_loss / len(validation_dataloader)\n    \n    \n    \n    # Measure how long the validation run took:\n    \n    validation_time = format_time(time.time() - t0)\n    \n    print('  Validation Loss: {0:.2f}'.format(avg_val_loss))\n    print('  Validation took: {:}'.format(validation_time))\n\n    # Record all statistics from this epoch.\n    \n    training_stats.append(\n        {\n            'epoch': epoch_i + 1,\n            'Training Loss': avg_train_loss,\n            'Valid. Loss': avg_val_loss,\n            'Valid. Accur.': avg_val_accuracy,\n            'Val_F1' : avg_val_f1,\n            'Training Time': training_time,\n            'Validation Time': validation_time\n        }\n    )\n\nprint('')\nprint('Training complete!')\n\nprint('Total training took {:} (h:mm:ss)'.format(format_time(time.time()-total_t0)))","metadata":{"execution":{"iopub.status.busy":"2022-03-03T18:47:28.547619Z","iopub.execute_input":"2022-03-03T18:47:28.547917Z","iopub.status.idle":"2022-03-03T18:48:25.205980Z","shell.execute_reply.started":"2022-03-03T18:47:28.547884Z","shell.execute_reply":"2022-03-03T18:48:25.204948Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}